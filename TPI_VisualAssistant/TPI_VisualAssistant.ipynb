{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Trabajo Integrador Técnicas de procesamiento de imágenes\n",
        "\n",
        "## IFTS 24\n",
        "\n",
        "Profesor: Matías Barreto\n",
        "\n",
        "Alumna Gabriela Pari Vaca\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Asistente por voz\n",
        "\n",
        "Descripción del proyecto\n",
        "\n",
        "El proyecto consiste en una interfaz que permite al usuario interactuar con imagen y voz. Mediante la camara web se podrá sacar una foto y hacerle una pregunta por voz, la respuesta también será mediante la voz.\n",
        "\n",
        "Tecnologías usadas\n",
        "- PaliGemma\n",
        "- Gradio\n",
        "- Torch\n",
        "- Transformers de Hugging Face\n",
        "- gTTS(Google Text-to-Speech)\n"
      ],
      "metadata": {
        "id": "mYaA9cKsY_Pv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalación e importación de librerías"
      ],
      "metadata": {
        "id": "iVenzUt8aTjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n",
        "!pip install torch transformers\n",
        "!pip install Pillow\n",
        "!pip install gTTS"
      ],
      "metadata": {
        "id": "W_0c-dSEZFgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "from transformers import AutoProcessor, PaliGemmaForConditionalGeneration, pipeline\n",
        "from PIL import Image\n",
        "from gtts import gTTS\n",
        "import os"
      ],
      "metadata": {
        "id": "hkFnrtZDY9l0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Token\n",
        "Para el uso de la libreria de Paligemma que se encuentra en HuggingFace.\n",
        "\n",
        "Se necesita tener creado un usuario con las credenciales. https://huggingface.co/settings/tokens\n",
        "\n",
        "Para el uso de este notebook se puede ir a la sección de \"secrets\" crear una clave con el nombre de 'HF_TOKEN'. Otra opción es usar el código de \"notebook_login\" donde se copia el token sin guardarlo"
      ],
      "metadata": {
        "id": "4FvWNi9IBSKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "#notebook_login(hf_token)\n",
        "login(hf_token)"
      ],
      "metadata": {
        "id": "N3m7u0RLklis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Configuración\n",
        "\n",
        "- Cargar el modelo VLM(visual-language-model) de Paligemma 3b\n",
        "- Configurar la máquina para que use GPU\n",
        "- Speech-to-Text"
      ],
      "metadata": {
        "id": "grK6qt_JZAlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Cargando modelo PaliGemma\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
        "model_id = \"google/paligemma-3b-mix-224\"\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "pali_model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=model_dtype,\n",
        "    device_map=device,\n",
        "    revision=\"bfloat16\",\n",
        ").eval()\n",
        "\n",
        "# --- Pipeline de Speech-to-Text (Whisper) ---\n",
        "print(\"Cargando modelo Whisper (Speech-to-Text)...\")\n",
        "# Usamos un modelo más pequeño para un inicio más rápido. Puedes cambiar a \"openai/whisper-large-v3\" para mayor precisión.\n",
        "asr_pipeline = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base\", device=device)\n",
        "\n",
        "print(\"Modelos cargados\")\n"
      ],
      "metadata": {
        "id": "yt58Hj8HZAlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Función Principal"
      ],
      "metadata": {
        "id": "zUUXaOZVZBKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def procesar_foto_y_voz(pil_image, audio_filepath):\n",
        "    \"\"\"\n",
        "    Orquesta todo el proceso:\n",
        "    1. Transcribe la voz a texto.\n",
        "    2. Hace la pregunta sobre la imagen al modelo PaliGemma.\n",
        "    3. Convierte la respuesta de texto a voz.\n",
        "\n",
        "    \"\"\"\n",
        "    # Validar entradas\n",
        "    if pil_image is None:\n",
        "        error_msg = \"ERROR: Por favor, toma una foto primero.\"\n",
        "        gTTS(text=error_msg, lang='es').save(\"error.mp3\")\n",
        "        return error_msg, \"error.mp3\"\n",
        "\n",
        "    if audio_filepath is None:\n",
        "        error_msg = \"ERROR: Por favor, graba una pregunta.\"\n",
        "        gTTS(text=error_msg, lang='es').save(\"error.mp3\")\n",
        "        return error_msg, \"error.mp3\"\n",
        "\n",
        "    try:\n",
        "        # Paso 1: Transcribir la voz a texto\n",
        "        print(f\"Transcribiendo audio desde: {audio_filepath}\")\n",
        "        transcripcion = asr_pipeline(audio_filepath)\n",
        "        pregunta_texto = transcripcion['text']\n",
        "        print(f\"Texto transcrito: '{pregunta_texto}'\")\n",
        "\n",
        "        if not pregunta_texto:\n",
        "            error_msg = \"No pude entender la pregunta. Por favor, habla más claro.\"\n",
        "            gTTS(text=error_msg, lang='es').save(\"error.mp3\")\n",
        "            return error_msg, \"error.mp3\"\n",
        "\n",
        "        # Paso 2: Usar PaliGemma para responder la pregunta sobre la imagen\n",
        "        prompt = pregunta_texto\n",
        "        inputs = processor(text=prompt, images=pil_image, return_tensors=\"pt\").to(pali_model.device)\n",
        "        reply = pali_model.generate(**inputs, max_new_tokens=100)\n",
        "        respuesta_texto = processor.decode(reply[0], skip_special_tokens=True)\n",
        "        respuesta_limpia = respuesta_texto.split(prompt)[-1].strip()\n",
        "        print(f\"Respuesta del modelo: '{respuesta_limpia}'\")\n",
        "\n",
        "        # Paso 3: Convertir la respuesta de texto a voz\n",
        "        output_audio_path = \"respuesta.mp3\"\n",
        "        tts = gTTS(text=respuesta_limpia, lang='es', slow=False)\n",
        "        tts.save(output_audio_path)\n",
        "        print(f\"Audio de respuesta guardado en: {output_audio_path}\")\n",
        "\n",
        "        # Devolver la respuesta en texto y la ruta al archivo de audio de respuesta\n",
        "        return respuesta_limpia, output_audio_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Ocurrió un error general: {e}\")\n",
        "        error_msg = f\"Lo siento, ocurrió un error: {e}\"\n",
        "        gTTS(text=\"Ocurrió un error inesperado\", lang='es').save(\"error.mp3\")\n",
        "        return error_msg, \"error.mp3\"\n"
      ],
      "metadata": {
        "id": "83kgBDH6ZBKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Creación de la Interfaz\n",
        "\n"
      ],
      "metadata": {
        "id": "Sd3I3g38jVDJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMAt8kXQYZ8a"
      },
      "outputs": [],
      "source": [
        "with gr.Blocks(theme=gr.themes.Soft(), title=\"Asistente Visual por Voz\") as demo:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        # Asistente Visual por Voz\n",
        "        Usa tu cámara para tomar una foto y luego graba una pregunta en español sobre ella.\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            image_input = gr.Image(label=\"Paso 1: Toma una foto\", sources=[\"webcam\"], type=\"pil\", elem_id=\"camera-container\")\n",
        "            audio_input = gr.Audio(label=\"Paso 2: Graba tu pregunta\", sources=[\"microphone\"], type=\"filepath\", elem_id=\"voice-recorder-container\")\n",
        "            submit_button = gr.Button(\"Procesar\", variant=\"primary\")\n",
        "\n",
        "        with gr.Column():\n",
        "            # Salidas\n",
        "            output_text = gr.Textbox(label=\"Respuesta en Texto\", lines=8, interactive=False)\n",
        "            output_audio = gr.Audio(label=\"Respuesta en Audio\", type=\"filepath\", autoplay=True)\n",
        "\n",
        "    # Conectar el botón a la función principal\n",
        "    submit_button.click(\n",
        "        fn=procesar_foto_y_voz,\n",
        "        inputs=[image_input, audio_input],\n",
        "        outputs=[output_text, output_audio]\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch() # debug=True te ayuda a ver errores en la consola\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Próximos pasos a seguir\n",
        "\n",
        "Mi idea de esta aplicación es aumentar la accesibilidad para los usuarios que tienen dificutad visual. El módulo JS se debe agregar con el parametro de js en gradio.\n",
        "\n"
      ],
      "metadata": {
        "id": "EFAjhHwsvveN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modulo JS para agregar mayor accecibilidad\n",
        "js = \"\"\"\n",
        "// Función para simular un clic en un botón dentro de un contenedor específico\n",
        "console.log(\"se ingreso\");\n",
        "function clickButtonInContainer(containerId) {\n",
        "    // Buscamos el contenedor por el ID que le dimos en Python (ej: #camera-container)\n",
        "    const container = document.querySelector(containerId);\n",
        "    console.log(\"clic container\");\n",
        "    if (container) {\n",
        "        // Buscamos el PRIMER botón que se encuentre DENTRO de ese contenedor\n",
        "        const button = container.querySelector(\"button\");\n",
        "        if (button) {\n",
        "            // Si encontramos el botón, le hacemos clic\n",
        "            button.click();\n",
        "            console.log(`Botón clickeado dentro de: ${containerId}`);\n",
        "        } else {\n",
        "            console.log(`No se encontró un botón dentro de: ${containerId}`);\n",
        "        }\n",
        "    } else {\n",
        "        console.log(`No se encontró el contenedor: ${containerId}`);\n",
        "    }\n",
        "}\n",
        "\n",
        "// Agregamos un \"escuchador\" de eventos para las pulsaciones de teclas en toda la página\n",
        "document.addEventListener(\"keydown\", (event) => {\n",
        "    console.log(\"se ingreso addeventListener\");\n",
        "\n",
        "    // Si se presiona la tecla \"f\"\n",
        "    if (event.key.toLowerCase() === \"f\") {\n",
        "        // Prevenimos la acción por defecto del navegador (como abrir la búsqueda)\n",
        "        event.preventDefault();\n",
        "        // Llamamos a nuestra función para hacer clic en el botón de la cámara\n",
        "        clickButtonInContainer(\"#camera-container\");\n",
        "    }\n",
        "\n",
        "    // Si se presiona la tecla \"v\"\n",
        "    if (event.key.toLowerCase() === \"v\") {\n",
        "        event.preventDefault();\n",
        "        // Llamamos a nuestra función para hacer clic en el botón de grabar voz\n",
        "        clickButtonInContainer(\"#voice-recorder-container\");\n",
        "    }\n",
        "});\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "hTqlVAXZU_NF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Links\n",
        "\n",
        "https://huggingface.co/google/paligemma-3b-mix-224"
      ],
      "metadata": {
        "id": "vKh9h_iiEqYk"
      }
    }
  ]
}